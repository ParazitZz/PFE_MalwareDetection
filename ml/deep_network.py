import sklearn
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation
from tensorflow.keras.optimizers import Adam, SGD
from tensorflow.keras import backend as K
from matplotlib import pyplot as plt
import matplotlib as mpl
from helpers import NNData, load_observations, split_data, measure_performance
from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter

from csv import writer
from os import mkdir
from os.path import isdir, isfile, join
from time import time
import pickle
import custometrics


input_observations = 'Drebin-Dataset/observations-6count'

train_pct = .8
observation_batches = 20
runs = 1
nbr_classes = 11

dump_directory = 'results'
svm_models_path = 'svm_models.csv'
nn_models_path = 'nn_models.csv'
rf_models_path = 'rf_models.csv'
lr_models_path = 'lr_models.csv'

colors = plt.rcParams['axes.prop_cycle'].by_key()['color']
mpl.rcParams['figure.figsize'] = (12, 10)

def nn_train():
    print('Using neural network')
    print("Num GPUs Available: ", len(tf.config.experimental.list_physical_devices('GPU')))

    data = NNData(input_observations, train_split=0.7, tv_split=0.5, nbr_classes=nbr_classes)
    data.data_preprocessing(calcul=False, inbalanced=False)
    METRICS = [
        tf.keras.metrics.CategoricalAccuracy(name='accuracy'),
        # tf.keras.metrics.Precision(name='precision'),
        # tf.keras.metrics.Recall(name='recall'),
        # tf.keras.metrics.AUC(name='auc'),
        custometrics.precision,
        custometrics.recall,
        custometrics.f1,
        custometrics.fbeta,
        custometrics.specificity,
        custometrics.negative_predictive_value,
        custometrics.matthews_correlation_coefficient,
        custometrics.equal_error_rate
    ]

    model = Sequential()
    model.add(Dense(512, input_dim=data.get_input_dim()))
    model.add(Activation('relu'))
    # model.add(Dense(1024))
    # model.add(Activation('relu'))
    model.add(Dense(512))
    model.add(Activation('relu'))
    model.add(Dense(256))
    model.add(Activation('relu'))
    model.add(Dense(nbr_classes))
    model.add(Activation('softmax'))
    adam = Adam(lr=1e-2, beta_1=.9, beta_2=.999, epsilon=1e-8, decay=.01)
    sgd = SGD(learning_rate=0.01, momentum=0.0, decay=0.01)
    model.compile(loss=tf.keras.losses.CategoricalCrossentropy(), optimizer=adam, metrics=METRICS)

    batch_size = 256
    # history = model.fit_generator(data.training_generator(observation_batches), steps_per_epoch=observation_batches, epochs=30)
    # for epoch in range(2):
    #     print("epoch = ", epoch)
    #     for batch in data.training_generator(observation_batches):
    #         history = model.fit(batch[0]/data.array_normalization, batch[1],batch_size=64)
    #batch_size = 2500
    # history = model.fit(data.training_generator(batch_size=batch_size), steps_per_epoch=2, epochs=30, batch_size=batch_size,
    #                     validation_data=data.validation_generator(batch_size=batch_size), validation_batch_size=batch_size,
    #                     validation_steps=len(data.validation_paths)*5000/batch_size, class_weight=data.class_weights)
    # history = model.fit(data.training_generator(batch_size=250), steps_per_epoch=20, epochs=60, batch_size=250,
    #                     validation_data=data.validation_generator(batch_size=250), validation_batch_size=250,
    #                     validation_steps=len(data.validation_paths) * 5000 / 250)
    history = model.fit(data.training_generator(batch_size=batch_size), steps_per_epoch=128, epochs=60,
                        batch_size=batch_size,
                        validation_data=data.validation_generator(batch_size=batch_size),
                        validation_batch_size=batch_size,
                        validation_steps=len(data.validation_paths) / batch_size,
                        class_weight=data.class_weights)
    x_te, y_te = data.get_test_data()
    loss_and_metrics = model.evaluate(x_te, y_te)
    print(loss_and_metrics)
    tpr, fpr = measure_performance(model.predict(x_te).argmax(axis=1), y_te.argmax(axis=1))
    print('True positive rate', tpr, 'False positive rate', fpr)
    id = str(int(time()))
    model.save(join(dump_directory, 'nn', id))
    with open(join(dump_directory,'nn',id,'history'), 'wb') as file_pi:
        pickle.dump(history.history, file_pi)
    if isfile(nn_models_path):
        with open(nn_models_path, 'a') as file:
            models_writer = writer(file)
            models_writer.writerow([id, tpr, fpr])
    else:
        with open(nn_models_path, 'w') as file:
            models_writer = writer(file)
            models_writer.writerow(['id', 'tpr', 'fpr'])
            models_writer.writerow([id, tpr, fpr])
    plot_metrics(history)
    # plt.plot(history.history['loss'])
    # plt.plot(history.history['val_loss'])
    # plt.xlabel('Epoch')
    # plt.ylabel('Loss')
    # plt.show()
    plt.plot(history.history['accuracy'])
    plt.plot(history.history['val_accuracy'])
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.show()
    # plt.savefig(id + '.png')

def plot_metrics(history):
    def format_func(value, tick_number):
        return 1-value

    metrics = ['loss', 'accuracy','precision', 'recall','f1', 'fbeta', 'specificity', 'negative_predictive_value', 'matthews_correlation_coefficient', 'equal_error_rate']
    for n, metric in enumerate(metrics):
        name = metric.replace("_"," ").capitalize()
        ax = plt.subplot(5,2,n+1)
        plt.xlabel('Epoch')
        plt.ylabel(name)
        if metric in ['loss', 'equal_error_rate']:
            plt.plot(history.epoch, history.history[metric], color=colors[0], label='Train')
            plt.plot(history.epoch, history.history['val_' + metric],
                     color=colors[0], linestyle="--", label='Val')
            # plt.ylim([0, plt.ylim()[1]])
            plt.yscale('log')

        elif metric in ['auc']:
            plt.plot(history.epoch, history.history[metric], color=colors[0], label='Train')
            plt.plot(history.epoch, history.history['val_' + metric],
                     color=colors[0], linestyle="--", label='Val')
            plt.ylim([0.9,1])

        elif metric in ['accuracy', 'specificity', 'negative_predictive_value', 'f1', 'fbeta','matthews_correlation_coefficient', 'recall', 'precision']:
            plt.plot(history.epoch, list(map((lambda x: 1-x),history.history[metric])) , color=colors[0], label='Train')
            plt.plot(history.epoch, list(map((lambda x: 1-x),history.history['val_' + metric])),
                     color=colors[0], linestyle="--", label='Val')
            plt.yscale('log')
            ax.yaxis.set_major_formatter(plt.FuncFormatter(format_func))
            plt.ylabel("1 - " + name)



        else:
            plt.plot(history.epoch, history.history[metric], color=colors[0], label='Train')
            plt.plot(history.epoch, history.history['val_' + metric],
                     color=colors[0], linestyle="--", label='Val')
            plt.ylim([0,1])

    plt.legend()
    plt.show()

def plot_roc(name, labels, predictions, **kwargs):
    fp, tp, _ = sklearn.metrics.roc_curve(labels, predictions)

    plt.plot(100*fp, 100*tp, label=name, linewidth=2, **kwargs)
    plt.xlabel('False positives [%]')
    plt.ylabel('True positives [%]')
    plt.xlim([-0.5,20])
    plt.ylim([80,100.5])
    plt.grid(True)
    ax = plt.gca()
    ax.set_aspect('equal')


if __name__ == '__main__':
    parser = ArgumentParser(
        description='Fits Machine Learning models onto the training datasets and outputs performance',
        formatter_class=ArgumentDefaultsHelpFormatter)
    parser.add_argument('-b', '--batches', type=int, default=observation_batches,
                        help='The amount of observation batches '
                             'to use. Be careful with large '
                             'numbers!')
    parser.add_argument('-d', '--data', type=str, default=input_observations, help='The directory which contains the '
                                                                                   'input training and test observations '
                                                                                   '(output of `gen_vectors.py`).')
    parser.add_argument('-r', '--runs', type=int, default=runs, help='Number of iterations of the training process.'
                                                                     'A number larger than 1 might diminish the'
                                                                     'influence of random initialization on model'
                                                                     'results.')
    parser.add_argument('-w', '--weight', type=int, default=2, help='Class weight for malware samples.')

    args = parser.parse_args()
    observation_batches = args.batches
    input_observations = args.data
    runs = args.runs
    nn_train()

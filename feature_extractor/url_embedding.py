import numpy as np
import pandas as pd

from collections import Counter
from gensim.models import Word2Vec

from keras.layers import Input
from keras.layers import LSTM
from keras.layers import RepeatVector
from keras.layers import Lambda
from keras.models import Model
from keras.models import load_model

import keras.backend as K

EMBSIZE = 250

def singleOneHot(c, dico, vocab_length):
    """ One-hot encodes one character 
    :param c: the character to encode
    :param dico: the one-hot encoding dictionary
    :param vocab_length: the size of the encoded vector

    :return: the one-hot encoded vector
    """
    arr = np.zeros((vocab_length, 1))
    arr[dico[c]] = 1
    return arr

class UrlEmbedding():
    """
    Class to generate char-level URL embedding as described in https://ieeexplore.ieee.org/document/8672248 with N=1
    """
    def __init__(self, series):
        assert isinstance(series, pd.Series), "Input type is not a pandas series : %r " % type(series)
        self.series = series
        self.n_urls = len(series)
        self.preprocess()
        self.getVocab()
        print(f"Vocab with {self.vocab_length} unique tokens : {self.vocab}")
        self.trainCharEmbedding()
    
    def preprocess(self):
        """ Slices each url at every character """
        self.series = self.series.str.strip().str.split('').str[1:-1]

    def getVocab(self):
        """ Retrieves the character vocabulary and defines some class attributes about it """
        try:
            return self.vocab
        except AttributeError:
            cnt = Counter()
            self.series.apply(lambda x: cnt.update(x))
            if '' in list(cnt):
                del cnt['']
            self.vocab_cnt = cnt
            self.vocab = list(self.vocab_cnt)
            self.vocab_length = len(self.vocab)
            return self.vocab

    def toOneHot(self):
        """ Defines the one-hot dictionary if not defined, then applies this one-hot encoding to each url of the series """
        try:
            return self.series.apply(lambda x: np.squeeze(np.asarray([singleOneHot(y, self.onehot_dict, self.vocab_length) for y in x]),axis=2))
        except AttributeError:
            self.onehot_dict = {}
            for i in range(self.vocab_length):
                self.onehot_dict[self.vocab[i]] = i
            return self.series.apply(lambda x: np.squeeze(np.asarray([singleOneHot(y, self.onehot_dict, self.vocab_length) for y in x]),axis=2))

    def trainCharEmbedding(self):
        print("Training the character embedding")
        self.embedding = Word2Vec(self.series, min_count=1, vector_size=EMBSIZE, workers=3, window=5, sg = 1)
    
    def toCharEmbedding(self, url):
        """ Encodes an URL by aggregating all the char embeddings
        :param url: url to char-embed

        :return: mean of all the character embeddings of the url
        """
        return np.asarray([self.embedding.wv[x] for x in url]).mean(axis=0)


class MultipleUrlEmbedding(UrlEmbedding):
    """ 
    Class generating multiple-Urls embeddings from the single urls embedding of UrlEmbedding class
    It selects the best discriminant features from each of the URLs with an LSTM autoencoder architecture
    This class manages the connexion with the single Url embedding, the training and inference of the model
    """
    def __init__(self, series, encoder_path):
        super().__init__(series) # Initiates UrlEmbedding mother class
        # On veut un embedding de plusieurs URLs de la taille de l'embedding d'un seul
        #self.model = self.define_LSTM_autoencoder(EMBSIZE, EMBSIZE)  
        self.encoder_path = encoder_path
        
    def getEncoderPath(self):
        return self.encoder_path

    def define_LSTM_autoencoder(self, input_dim, latent_dim):
        """ Definition of the LSTM autoencoder
        :param input_dim: dimension of the feature vectors of the model
        :param latent_dim: dimension wanted in compressed state

        :return sequence_autoencoder: the autoencoder model
        :return encoder: only the encoder model
        """
        def repeat(x):
            """ Function to tweak autoencoder to accept variable-length inputs """
            stepMatrix = K.ones_like(x[0][:,:,:1]) # matrix with ones, shaped as (batch, steps, 1)
            latentMatrix = K.expand_dims(x[1],axis=1) # latent vars, shaped as (batch, 1, latent_dim)
            return K.batch_dot(stepMatrix,latentMatrix)

        inputs = Input(shape=(None, input_dim)) # None (variable-length input), input_dim (Embedding size of an URL) 
        encoded = LSTM(latent_dim)(inputs) # Feed it to the LSTM

        decoded = Lambda(repeat)([inputs,encoded]) # We repeat the encoded vector as many times as there are URLs in the input
        decoded = LSTM(input_dim, return_sequences=True)(decoded) # decoder LSTM

        sequence_autoencoder = Model(inputs, decoded) # Full autoencoder
        encoder = Model(inputs, encoded) # Only encoder

        sequence_autoencoder.compile(optimizer='adam', loss='mse')
        print(sequence_autoencoder.summary())
        return sequence_autoencoder, encoder

    def train_generator(self): 
        """ Samples n urls between high and low for the autoencoder to recreate
        """
        while True:
            number_of_urls = np.random.randint(low=5, high=100)
            sample = np.random.choice(self.series, number_of_urls)
            embedded = []
            for url in sample:
                embedded.append(self.toCharEmbedding(url))
            embedded = np.asarray(embedded)
            embedded = np.expand_dims(embedded, axis=0)
            yield embedded, embedded

    def train_LSTM_autoencoder(self, autoencoder, encoder, steps=100, epochs=10):
        """
        Trains the lstm autoencoder
        """
        print("Start of the autoencoder training")
        autoencoder.fit(
            self.train_generator(),
            steps_per_epoch=steps, 
            epochs=epochs, 
            verbose=1)
        encoder.save(self.encoder_path) # Save encoder for inference later

    def UrlsToEmb(self, encoder, urls):
        """ Takes a list of Urls and transforms it into the final embedding
        :param encoder: trained encoder for dimension reduction
        :param urls: list of urls to embed

        :return result: embedded list of url
        """
        embedded = []
        for url in urls:
            embedded.append(self.toCharEmbedding(url))
        embedded = np.asarray(embedded)
        embedded = np.expand_dims(embedded, axis=0)
        
        result = encoder.predict(embedded)
        result = np.squeeze(result)
        return result


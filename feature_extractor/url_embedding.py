import numpy as np
import pandas as pd

from collections import Counter
from gensim.models import Word2Vec

from keras.models import Sequential
from keras.layers import LSTM
from keras.layers import Dense
from keras.layers import RepeatVector
from keras.layers import TimeDistributed

EMBSIZE = 200
END_OF_URL_TOKEN = '|'
NULLMASK_TOKEN = np.zeros((EMBSIZE,))


def singleOneHot(c, dico, vocab_length):
    arr = np.zeros((vocab_length, 1))
    arr[dico[c]] = 1
    return arr

class UrlEmbedding():
    """
    Classe pour générer les embeddings d'URL
    """
    def __init__(self, series):
        assert isinstance(series, pd.Series), "Input type is not a pandas series : %r " % type(series)
        self.series = series
        self.n_urls = len(series)
        self.preprocess()
        self.getVocab()
        print(f"Vocab : {self.vocab}")
        self.trainCharEmbedding()
    
    def preprocess(self):
        assert END_OF_URL_TOKEN not in vocab, "END_OF_URL_TOKEN %r found in vocabulary. Change it." % END_OF_URL_TOKEN
        self.vocab = self.vocab + ['|']
        self.series = self.series + '|'
        self.series = self.series.str.strip().str.split('').str[1:-1]

    def getVocab(self):
        try:
            return self.vocab
        except AttributeError:
            cnt = Counter()
            self.series.apply(lambda x: cnt.update(x))
            if '' in list(cnt):
                del cnt['']
            self.vocab_cnt = cnt
            self.vocab = list(self.vocab_cnt)
            self.vocab_length = len(self.vocab)
            return self.vocab

    def toOneHot(self):
        try:
            return self.series.apply(lambda x: np.squeeze(np.asarray([singleOneHot(y, self.onehot_dict, self.vocab_length) for y in x]),axis=2))
        except AttributeError:
            self.onehot_dict = {}
            for i in range(self.vocab_length):
                self.onehot_dict[self.vocab[i]] = i
            return self.series.apply(lambda x: np.squeeze(np.asarray([singleOneHot(y, self.onehot_dict, self.vocab_length) for y in x]),axis=2))

    def trainCharEmbedding(self):
        print("Training the character embedding")
        self.embedding = Word2Vec(self.series, min_count=1, vector_size=EMBSIZE, workers=3, window=5, sg = 1)
    
    def toCharEmbedding(self, url):
        return np.asarray([self.embedding.wv[x] for x in url]).mean(axis=0)


class MultipleUrlEmbedding(UrlEmbedding):
    def __init__(self, series):
        super().__init__(series)
        self.preprocessForAE()
        self.model = self.autoencoder(5, 200)

    def preprocessForAE(self, ):
        self.maxlength = self.series.apply(lambda x: len(x)).max()
        assert self.maxlength <= 200, 'Désolé mais tu vas devoir coder le cas des urls de plus de 200 char bro'
            
        exit(-1)

    def autoencoder(self, timesteps, n_features):
        model = Sequential()
        model.add(LSTM(128, activation='relu', input_shape=(timesteps,n_features), return_sequences=True))
        model.add(LSTM(64, activation='relu', return_sequences=False))
        model.add(RepeatVector(timesteps))
        model.add(LSTM(64, activation='relu', return_sequences=True))
        model.add(LSTM(128, activation='relu', return_sequences=True))
        model.add(TimeDistributed(Dense(n_features)))
        model.compile(optimizer='adam', loss='mse')
        print(model.summary())




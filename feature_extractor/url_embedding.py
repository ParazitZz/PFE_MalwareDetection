import numpy as np
import pandas as pd

from collections import Counter
from gensim.models import Word2Vec

from keras.layers import Input
from keras.layers import LSTM
from keras.layers import RepeatVector
from keras.layers import TimeDistributed
from keras.layers import Lambda
from keras.models import Model
from keras.models import load_model

import keras.backend as K


EMBSIZE = 200
END_OF_URL_TOKEN = '|'
NULLMASK_TOKEN = np.zeros((EMBSIZE,))


def singleOneHot(c, dico, vocab_length):
    arr = np.zeros((vocab_length, 1))
    arr[dico[c]] = 1
    return arr

class UrlEmbedding():
    """
    Classe pour générer les embeddings d'URL
    """
    def __init__(self, series):
        assert isinstance(series, pd.Series), "Input type is not a pandas series : %r " % type(series)
        self.series = series
        self.n_urls = len(series)
        self.preprocess()
        self.getVocab()
        print(f"Vocab : {self.vocab}")
        self.trainCharEmbedding()
    
    def preprocess(self):
        self.series = self.series.str.strip().str.split('').str[1:-1]

    def getVocab(self):
        try:
            return self.vocab
        except AttributeError:
            cnt = Counter()
            self.series.apply(lambda x: cnt.update(x))
            if '' in list(cnt):
                del cnt['']
            self.vocab_cnt = cnt
            self.vocab = list(self.vocab_cnt)
            self.vocab_length = len(self.vocab)
            return self.vocab

    def toOneHot(self):
        try:
            return self.series.apply(lambda x: np.squeeze(np.asarray([singleOneHot(y, self.onehot_dict, self.vocab_length) for y in x]),axis=2))
        except AttributeError:
            self.onehot_dict = {}
            for i in range(self.vocab_length):
                self.onehot_dict[self.vocab[i]] = i
            return self.series.apply(lambda x: np.squeeze(np.asarray([singleOneHot(y, self.onehot_dict, self.vocab_length) for y in x]),axis=2))

    def trainCharEmbedding(self):
        print("Training the character embedding")
        self.embedding = Word2Vec(self.series, min_count=1, vector_size=EMBSIZE, workers=3, window=5, sg = 1)
    
    def toCharEmbedding(self, url):
        return np.asarray([self.embedding.wv[x] for x in url]).mean(axis=0)


class MultipleUrlEmbedding(UrlEmbedding):
    def __init__(self, series, encoder_path):
        super().__init__(series) # Initiates UrlEmbedding mother class
        # self.preprocessForAE()
        # On veut un embedding de plusieurs URLs de la taille de l'embedding d'un seul
        self.model = self.define_LSTM_autoencoder(EMBSIZE, EMBSIZE)  
        self.encoder_path = encoder_path
        

    def preprocessForAE(self):
        pass

    def define_LSTM_autoencoder(self, input_dim, latent_dim):
        def repeat(x):
            """ Function to tweak autoencoder to accept variable-length inputs """
            stepMatrix = K.ones_like(x[0][:,:,:1]) # matrix with ones, shaped as (batch, steps, 1)
            latentMatrix = K.expand_dims(x[1],axis=1) # latent vars, shaped as (batch, 1, latent_dim)
            return K.batch_dot(stepMatrix,latentMatrix)

        inputs = Input(shape=(None, input_dim)) # None (variable-length input), input_dim (Embedding size of an URL) 
        encoded = LSTM(latent_dim)(inputs) # Feed it to the LSTM

        decoded = Lambda(repeat)([inputs,encoded]) # We repeat the encoded vector as many times as there are URLs in the input
        decoded = LSTM(input_dim, return_sequences=True)(decoded) # decoder LSTM

        sequence_autoencoder = Model(inputs, decoded) # Full autoencoder
        encoder = Model(inputs, encoded) # Only encoder

        sequence_autoencoder.compile(optimizer='adam', loss='mse')
        print(sequence_autoencoder.summary())
        return sequence_autoencoder, encoder

    def train_generator(self):
        while True:
            number_of_urls = np.random.randint(low=2, high=5)
            sample = np.random.choice(self.series, number_of_urls)
            embedded = []
            for url in sample:
                embedded.append(self.toCharEmbedding(url))
            embedded = np.asarray(embedded)
            embedded = np.expand_dims(embedded, axis=0)
            yield embedded, embedded

    def train_LSTM_autoencoder(self, autoencoder, encoder, steps=100, epochs=10):
        """
        All input_shapes must be the same inside a mini-batch. 
        We train autoencoder with a train_generator to manage the variable_length input
        and feed same-sized batches to the model
        """
        autoencoder.fit(
            self.train_generator(),
            steps_per_epoch=steps, 
            epochs=epochs, 
            verbose=1)
        encoder.save(self.encoder_path) # Save encoder for inference later


import numpy as np
import pandas as pd

from collections import Counter
from gensim.models import Word2Vec

from keras.layers import Input
from keras.layers import LSTM
from keras.layers import Dense
from keras.layers import RepeatVector
from keras.layers import Lambda
from keras.models import Model
from keras.models import load_model
from keras.losses import MeanAbsoluteError, SparseCategoricalCrossentropy
from keras.optimizers import Adam
from urllib.parse import urlparse, urlsplit
import tensorflow as tf

import keras.backend as K

from scipy.stats import skew, kurtosis

EMBSIZE = 51


def my_loss_fn(y_true, y_pred):
    abs_difference = tf.abs(y_true - y_pred)
    exp_loss = tf.exp(abs_difference)
    return tf.reduce_mean(exp_loss, axis=-1)  # Note the `axis=-1`

def ZeroPad(arr, N):
    current_shape = arr.shape[0]
    arr = np.pad(arr, ((0,N-current_shape), (0, 0)), mode='constant')
    return arr 


def gini_coefficient(x):
    """Compute Gini coefficient of array of values"""
    diffsum = 0
    for i, xi in enumerate(x[:-1], 1):
        diffsum += np.sum(np.abs(xi - x[i:]))
    return diffsum / (len(x)**2 * np.mean(x))

class ParsedUrl():
    """
    Parsed Url class to simplify url cutting 
    """
    def __init__(self, url):
        assert isinstance(url, str), "url is not a string: %r" % type(url)
        if not ('//' in url):
            url = '//' + url
        self.url = url
        self.parsed = urlparse(self.url)
        self.protocol = self.parsed.scheme
        self.domain = self.parsed.netloc
        self.path = self.parsed.path + self.parsed.params + self.parsed.query + self.parsed.fragment

    def print(self):
        print(f"ParsedUrl Object protocol {self.protocol} domain {self.domain} path {self.path}")


def singleOneHot(c, dico, vocab_length):
    """ One-hot encodes one character 
    :param c: the character to encode
    :param dico: the one-hot encoding dictionary
    :param vocab_length: the size of the encoded vector

    :return: the one-hot encoded vector
    """
    arr = np.zeros((vocab_length, 1))
    arr[dico[c]] = 1
    return arr

class UrlEmbedding():
    """
    Class to generate char-level URL embedding as described in https://ieeexplore.ieee.org/document/8672248 
    """
    def __init__(self, N, series, save_path):
        assert isinstance(series, pd.Series), "Input type is not a pandas series : %r " % type(series)
        assert (N == 1) or (N == 3), "N must be equal to 1 or 3"
        self.N = N
        if self.N == 3:
            global EMBSIZE
            EMBSIZE = int(EMBSIZE / N)
        self.series = series
        self.n_urls = len(series)
        self.preprocess()
        self.getVocab()
        print(f"Vocab with {self.vocab_length} unique tokens : {self.vocab}")
        self.embedding_path = save_path + '/embedding.model'
        self.trainCharEmbedding()
    
    def preprocess(self):
        """ Slices each url at every character """
        self.series = self.series.str.strip().str.split('').str[1:-1]

    def getVocab(self):
        """ Retrieves the character vocabulary and defines some class attributes about it """
        try:
            return self.vocab
        except AttributeError:
            cnt = Counter()
            self.series.apply(lambda x: cnt.update(x))
            if '' in list(cnt):
                del cnt['']
            self.vocab_cnt = cnt
            self.vocab = list(self.vocab_cnt)
            self.vocab_length = len(self.vocab)
            return self.vocab

    def toOneHot(self):
        """ Defines the one-hot dictionary if not defined, then applies this one-hot encoding to each url of the series """
        try:
            return self.series.apply(lambda x: np.squeeze(np.asarray([singleOneHot(y, self.onehot_dict, self.vocab_length) for y in x]),axis=2))
        except AttributeError:
            self.onehot_dict = {}
            for i in range(self.vocab_length):
                self.onehot_dict[self.vocab[i]] = i
            return self.series.apply(lambda x: np.squeeze(np.asarray([singleOneHot(y, self.onehot_dict, self.vocab_length) for y in x]),axis=2))

    def trainCharEmbedding(self):
        try:
            self.embedding =  Word2Vec.load(self.embedding_path)
            print(f"Using the pre-trained embedding model at {self.embedding_path}")
        except FileNotFoundError:
            print("Training the character embedding")
            self.embedding = Word2Vec(self.series, min_count=1, vector_size=EMBSIZE, workers=3, window=5, sg = 1)
            self.embedding.save(self.embedding_path)
    
    def toCharEmbedding(self, url):
        """ Encodes an URL by aggregating all the char embeddings
        :param url: url to char-embed

        :return: mean of all the character embeddings of the url
        """
        url = "".join(url)
        url = url.replace("[", "")
        url = url.replace("]", "")
        if self.N == 1:
            vector = np.asarray([self.embedding.wv[x] for x in url]).mean(axis=0)
            return (vector/np.linalg.norm(vector, ord=2))
        if self.N == 3:
            parsed = ParsedUrl(url)
            if len(parsed.protocol) > 0:
                protocol = np.asarray([self.embedding.wv[x] for x in parsed.protocol]).mean(axis=0)
                protocol = protocol/np.linalg.norm(protocol, 2)
            else:
                protocol = np.zeros((EMBSIZE,))
            if len(parsed.domain) > 0:
                domain = np.asarray([self.embedding.wv[x] for x in parsed.domain]).mean(axis=0)
                domain = domain/np.linalg.norm(domain, 2)
            else:
                domain = np.zeros((EMBSIZE,))
            if len(parsed.path) > 0:
                path = np.asarray([self.embedding.wv[x] for x in parsed.path]).mean(axis=0)
                path = path/np.linalg.norm(path, 2)
            else:
                path = np.zeros((EMBSIZE,))
            return np.concatenate([protocol, domain, path])


    def UrlsToEnc(self, urls):
        """ Takes a list of Urls and transforms it into the final embedding
        :param encoder: trained encoder for dimension reduction
        :param urls: list of urls to embed

        :return result: embedded list of url
        """
        embedded = []
        for url in urls:
            embedded.append(self.toCharEmbedding(url))
        embedded = np.asarray(embedded)
        try:
            if len(embedded) > 0:
                amin = embedded.min(axis=0)
                amax = embedded.max(axis=0)
                mean = embedded.mean(axis=0)
                median = np.apply_along_axis(np.median, 0, embedded)
                std = embedded.std(axis=0)
                skewval = np.apply_along_axis(skew, 0, embedded)
                kurtval = np.apply_along_axis(kurtosis, 0, embedded)
                gini = np.apply_along_axis(gini_coefficient, 0, embedded)
                gini = np.nan_to_num(gini)
                result = np.concatenate([amin, amax, mean, median, std, skewval, kurtval, gini])
            else:
                np.zeros((9*int((EMBSIZE/3)*3),))
        except ValueError:
            print("Urls : ")
            print(urls)
            print("Embedded : ")
            print(embedded)
            print(embedded.shape)
        return result 
        
        
        


class MultipleUrlEmbedding(UrlEmbedding):
    """ 
    Class generating multiple-Urls embeddings from the single urls embedding of UrlEmbedding class
    It selects the best discriminant features from each of the URLs with an LSTM autoencoder architecture
    This class manages the connexion with the single Url embedding, the training and inference of the model
    """
    def __init__(self, N, series, save_path, max_shape=None):
        super().__init__(N, series, save_path) # Initiates UrlEmbedding mother class
        # On veut un embedding de plusieurs URLs de la taille de l'embedding d'un seul
        #self.model = self.define_LSTM_autoencoder(EMBSIZE, EMBSIZE)
        self.LSTM = False
        if max_shape is None:
            self.LSTM = True
        else:
            assert isinstance(max_shape, int), "max_shape is not an integer"
            self.max_shape = max_shape  

        self.encoder_path = save_path + '/encoder.h5'
        # STORE THE CHARACTER EMBEDDING
        
    def getEncoderPath(self):
        return self.encoder_path

    def define_LSTM_autoencoder(self, input_dim, latent_dim):
        """ Definition of the LSTM autoencoder
        :param input_dim: dimension of the feature vectors of the model
        :param latent_dim: dimension wanted in compressed state

        :return sequence_autoencoder: the autoencoder model
        :return encoder: only the encoder model
        """
        def repeat(x):
            """ Function to tweak autoencoder to accept variable-length inputs """
            stepMatrix = K.ones_like(x[0][:,:,:1]) # matrix with ones, shaped as (batch, steps, 1)
            latentMatrix = K.expand_dims(x[1],axis=1) # latent vars, shaped as (batch, 1, latent_dim)
            return K.batch_dot(stepMatrix,latentMatrix)

        inputs = Input(shape=(None, input_dim)) # None (variable-length input), input_dim (Embedding size of an URL) 
        encoded = LSTM(latent_dim)(inputs) # Feed it to the LSTM

        decoded = Lambda(repeat)([inputs,encoded]) # We repeat the encoded vector as many times as there are URLs in the input
        decoded = LSTM(input_dim, return_sequences=True)(decoded) # decoder LSTM

        sequence_autoencoder = Model(inputs, decoded) # Full autoencoder
        encoder = Model(inputs, encoded) # Only encoder

        opt = Adam(learning_rate=0.01)
        sequence_autoencoder.compile(optimizer=opt, loss=my_loss_fn)
        print(sequence_autoencoder.summary())
        return sequence_autoencoder, encoder

    def define_autoencoder(self, input_dim, number_of_urls, latent_dim):
        input_vec = Input(shape=(number_of_urls * input_dim,))
        # "encoded" is the encoded representation of the input
        encoded = Dense(latent_dim, activation='tanh')(input_vec)
        # "decoded" is the lossy reconstruction of the input
        decoded = Dense(number_of_urls * input_dim)(encoded)

        autoencoder = Model(input_vec, decoded) # Full autoencoder
        encoder = Model(input_vec, encoded) # Only encoder

        opt = Adam(learning_rate=0.01)
        autoencoder.compile(optimizer=opt, loss=MeanAbsoluteError())
        print(autoencoder.summary())
        return autoencoder, encoder


    def train_generator_LSTM(self): 
        """ Samples n urls between high and low for the autoencoder to recreate
        """
        while True:
            number_of_urls = np.random.randint(low=5, high=100)
            sample = np.random.choice(self.series, number_of_urls)
            embedded = []
            for url in sample:
                embedded.append(self.toCharEmbedding(url))
            embedded = np.asarray(embedded)
            embedded = np.expand_dims(embedded, axis=0)
            yield embedded, embedded

    def train_generator(self):
        """ Samples n urls between high and low for the autoencoder to recreate
        """
        while True:
            number_of_urls = np.random.randint(low=5, high=100)
            sample = np.random.choice(self.series, number_of_urls)
            embedded = []
            for url in sample:
                embedded.append(self.toCharEmbedding(url))
            embedded = np.asarray(embedded)
            embedded = ZeroPad(embedded, self.max_shape).flatten()
            embedded = np.expand_dims(embedded, axis=0)
            yield embedded, embedded

    def train_LSTM_autoencoder(self, autoencoder, encoder, steps=100, epochs=10):
        """
        Trains the lstm autoencoder
        """
        print("Start of the lstm autoencoder training")
        history = autoencoder.fit(
            self.train_generator_LSTM(),
            steps_per_epoch=steps, 
            epochs=epochs, 
            verbose=1)
        pd.Series(history.history['loss']).plot()
        encoder.save(self.encoder_path) # Save encoder for inference later
        autoencoder.save(self.encoder_path)
    
    def train_autoencoder(self, autoencoder, encoder, steps=100, epochs=10):
        """
        Trains the autoencoder
        """
        print("Start of the autoencoder training")
        history = autoencoder.fit(
            self.train_generator(),
            steps_per_epoch=steps, 
            epochs=epochs, 
            verbose=1)
        pd.Series(history.history['loss']).plot()
        encoder.save(self.encoder_path) # Save encoder for inference later
        autoencoder.save(self.encoder_path)

    def UrlsToEmb_LSTM(self, encoder, urls):
        """ Takes a list of Urls and transforms it into the final embedding
        :param encoder: trained encoder for dimension reduction
        :param urls: list of urls to embed

        :return result: embedded list of url
        """
        embedded = []
        for url in urls:
            embedded.append(self.toCharEmbedding(url))
        embedded = np.asarray(embedded)
        embedded = np.expand_dims(embedded, axis=0)
        
        result = encoder.predict(embedded)
        result = np.squeeze(result)
        return result 
    
    def UrlsToEmb(self, encoder, urls):
        """ Takes a list of Urls and transforms it into the final embedding
        :param encoder: trained encoder for dimension reduction
        :param urls: list of urls to embed

        :return result: embedded list of url
        """
        embedded = []
        for url in urls:
            embedded.append(self.toCharEmbedding(url))
        embedded = np.asarray(embedded)
        embedded = ZeroPad(embedded, self.max_shape).flatten()
        embedded = np.expand_dims(embedded, axis=0)
        
        result = encoder.predict(embedded)
        result = np.squeeze(result)
        return result 

    def ForwardPass(self, autoencoder, urls):
        embedded = []
        for url in urls:
            embedded.append(self.toCharEmbedding(url))
        embedded = np.asarray(embedded)
        if not self.LSTM :
            embedded = ZeroPad(embedded, self.max_shape).flatten()
        embedded = np.expand_dims(embedded, axis=0)
        print("Avant : ")
        print(embedded.shape)
        print(embedded)

        result = autoencoder.predict(embedded)
        print("Après : ")
        print(result.shape)
        print(result)

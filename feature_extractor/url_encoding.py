from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter
from pickle import dump, load
from os import listdir, mkdir
from os.path import isfile, isdir, join
from urllib.parse import urlparse, urlsplit

import numpy as np
import pandas as pd

from url_embedding import *

#from tensorflow.keras.models import Sequential
#from tensorflow.keras.layers import Dense, Activation
#from tensorflow.keras.optimizers import Adam, SGD
#from tensorflow.keras import backend as K

from matplotlib import pyplot as plt

in_directory = 'Drebin-Dataset/feature_vectors'
in_directory = '../../Data/feature_vectors'
out_directory = 'feature_extractor'
out_file = 'urls.p'

class ParsedUrl():
    """
    Parsed Url class to simplify url cutting 
    """
    def __init__(self, url):
        assert isinstance(url, str), "url is not a string: %r" % type(url)
        if not ('//' in url):
            url = '//' + url
        self.url = url
        self.parsed = urlparse(self.url)
        self.protocol = self.parsed.scheme
        self.domain = self.parsed.netloc
        self.path = self.parsed.path + self.parsed.params + self.parsed.query + self.parsed.fragment

    def print(self):
        print(f"ParsedUrl Object protocol {self.protocol} domain {self.domain} path {self.path}")


def allf(features, line):
    # Add the distinct feature to 'all' set
    features.get('all').add(line[1])

def nonef(features, line):
    pass

def unknown(features, line):
    if len(line) == 2:
        print('Unknown feature : ', line[0] + '::' + line[1])
        if len((line[0] + line[1]).strip()):
            exit(1)

feature_set = {
    'feature': nonef,  # Hardware Components 72
    'permission': nonef,  # Requested Permission 3812
    'activity': nonef,  # App Components p1 185729
    'service_receiver': nonef,  # App Component p2 33222
    'provider': nonef,  # App Component p3# : 4513
    'intent': nonef,  # Filtered Intent 6379
    'call': nonef,  # Restricted Api Calls 733
    'real_permission': nonef,  # Used Permission 70
    'api_call': nonef,  # = Suspicious Api Calls 315
    'url': allf,  # Network Address 310488
}

def accumulate_features(in_dir=in_directory, out_dir=out_directory):
    if not isdir(out_dir):
        mkdir(out_dir)
    paths = [join(in_dir, f) for f in listdir(in_dir)]
    features = {'all': set()}
    for idx, path in enumerate(paths):
        with open(path, 'r') as file:
            lines = file.read().splitlines()
            lines = [sl.split('::') for sl in lines]
            for line in lines:
                switcher = feature_set.get(line[0], unknown)
                switcher(features, line)
    with open(join(out_dir, out_file), 'wb') as file:
        flatfeature = []
        flatfeature.extend(list(features.get('all')))
        dump(flatfeature, file)


def get_unique_URLs(out_dir=out_directory, out_file=out_file):
    """ Gets all unique URLs stored in the file """
    with open(join(out_dir, out_file), 'rb') as f:
        features = load(f) # Valeurs possibles des features 
    return pd.Series(data=features)


if __name__ == "__main__":
    if not isfile(out_file):
        accumulate_features(in_dir=in_directory, out_dir=out_directory)
    urls = get_unique_URLs(out_dir=out_directory, out_file=out_file)
    #print(cnt)
    #print("Most commons : ")
    #print(cnt.most_common(10))
    #print(f"Il y a {len(list(cnt))} unique elements : ")
    #print(list(cnt))
    
    emb = MultipleUrlEmbedding(urls, 'feature_extractor/encoder.h5') # On ne prend que 100 URLs
    autoencoder, encoder = emb.define_LSTM_autoencoder(200, 200)
    print(final_embedding)
    emb.train_LSTM_autoencoder(autoencoder, encoder, steps=1000, epochs=10)
    encoder = load_model(emb.getEncoderPath())
    final_embedding = emb.UrlsToEmb(encoder, ['https://facebook.com', '121.25.65.10', 'penelopecessac.fr', 'https://clients.boursorama.com', 'https://google.fr'])
    print(f"Final Embedding with shape {final_embedding.shape}: ")
    print(final_embedding)
    
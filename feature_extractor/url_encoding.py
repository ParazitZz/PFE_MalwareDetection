from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter
from pickle import dump, load
from os import listdir, mkdir
from os.path import isfile, isdir, join
import numpy as np
import pandas as pd

from url_embedding import *

#from tensorflow.keras.models import Sequential
#from tensorflow.keras.layers import Dense, Activation
#from tensorflow.keras.optimizers import Adam, SGD
from tensorflow.keras import backend as K

from matplotlib import pyplot as plt

in_directory = 'Drebin-Dataset/feature_vectors'
out_directory = 'feature_extractor'
out_file = 'urls.p'



def allf(features, line):
    # Add the distinct feature to 'all' set
    features.get('all').add(line[1])

def nonef(features, line):
    pass

def unknown(features, line):
    if len(line) == 2:
        print('Unknown feature : ', line[0] + '::' + line[1])
        if len((line[0] + line[1]).strip()):
            exit(1)

feature_set = {
    'feature': nonef,  # Hardware Components 72
    'permission': nonef,  # Requested Permission 3812
    'activity': nonef,  # App Components p1 185729
    'service_receiver': nonef,  # App Component p2 33222
    'provider': nonef,  # App Component p3# : 4513
    'intent': nonef,  # Filtered Intent 6379
    'call': nonef,  # Restricted Api Calls 733
    'real_permission': nonef,  # Used Permission 70
    'api_call': nonef,  # = Suspicious Api Calls 315
    'url': allf,  # Network Address 310488
}

def accumulate_features(in_dir=in_directory, out_dir=out_directory):
    if not isdir(out_dir):
        mkdir(out_dir)
    paths = [join(in_dir, f) for f in listdir(in_dir)]
    features = {'all': set()}
    for idx, path in enumerate(paths):
        with open(path, 'r') as file:
            lines = file.read().splitlines()
            lines = [sl.split('::') for sl in lines]
            for line in lines:
                switcher = feature_set.get(line[0], unknown)
                switcher(features, line)
    with open(join(out_dir, out_file), 'wb') as file:
        flatfeature = []
        flatfeature.extend(list(features.get('all')))
        dump(flatfeature, file)


def get_unique_URLs(out_dir=out_directory, out_file=out_file):
    """ Gets all unique URLs stored in the file """
    with open(join(out_dir, out_file), 'rb') as f:
        features = load(f) # Valeurs possibles des features 
    return pd.Series(data=features)


from keras.models import load_model

if __name__ == "__main__":
    if not isfile(out_file):
        accumulate_features(in_dir=in_directory, out_dir=out_directory)
    urls = get_unique_URLs(out_dir=out_directory, out_file=out_file)
    #print(cnt)
    #print("Most commons : ")
    #print(cnt.most_common(10))
    #print(f"Il y a {len(list(cnt))} unique elements : ")
    #print(list(cnt))
    
    max_shape = 100
    emb = MultipleUrlEmbedding(3, urls, 'strat_feature_vectors/1', max_shape=max_shape) # On ne prend que 100 URLs
    vect = emb.toCharEmbedding('app.dwap.com')
    print(f"Embedding shape is {vect.shape}")
    """
    test = ['https://facebook.com', '121.25.65.10', 'penelopecessac.fr', 'https://clients.boursorama.com', 'https://google.fr']
    enc = emb.UrlsToEnc(test)
    print(enc.shape)
    print(enc)
    #autoencoder, encoder = emb.define_LSTM_autoencoder(300, 1000)
    
    autoencoder, encoder = emb.define_autoencoder(EMBSIZE, max_shape, 1000)
    try:
        autoencoder = load_model(emb.getEncoderPath())
    except (FileNotFoundError, OSError):
        #emb.train_LSTM_autoencoder(autoencoder, encoder, steps=1000, epochs=20)
        emb.train_autoencoder(autoencoder, encoder, steps=1000, epochs=20)
        

    #final_embedding = emb.UrlsToEmb(encoder, ['https://facebook.com', '121.25.65.10', 'penelopecessac.fr', 'https://clients.boursorama.com', 'https://google.fr'])
    #print(f"Final Embedding with shape {final_embedding.shape}: ")
    #print(final_embedding)
    emb.ForwardPass(autoencoder, ['https://facebook.com', '121.25.65.10', 'penelopecessac.fr', 'https://clients.boursorama.com', 'https://google.fr'])
    """
    
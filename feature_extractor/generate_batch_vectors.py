#!/usr/bin/env python3
from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter
from csv import reader
from math import ceil
from pickle import dump, load
import os
from os import listdir, mkdir
from os.path import basename, isdir, join, splitext, isfile
import sys
from url_encoding import *
import pandas


def countf(feature, lines, observation):
    feature_head = feature.split('::')[0]
    nbr_feature = len([x for x in lines if feature_head == x[0:len(feature_head)]])
    observation.append(nbr_feature) # Ajoute le nombre de fois qu'il y a la feature dans observation


def allfeature(feature, lines, observation):
    # One-hot encoding of the feature
    if feature in lines:
        observation.append(1)
    else:
        observation.append(0)

def allurl(emb, encoder, lines, observation):
    urls = []
    for line in lines:
        sp = line.split('::')
        if sp[0] == 'url':
            urls.append(sp[1])
    if len(urls) > 0:
        [observation.append(x) for x in emb.UrlsToEmb(encoder, urls)]
    else: [observation.append(0) for i in range(EMBEDDING_SIZE)]

def nonef(feature, lines, observation):
    pass


def unknown(feature, lines, observation):
    print('Unknown feature', feature)

def new_lines_split(lines, split_symb, pos):
    new_lines = []
    for l in lines:
        base = l.split('::')
        value_split = base[1].split(split_symb)
        if len(value_split) > 1:
            new_lines.append(base[0] + '::' + value_split[pos])
    return new_lines


def new_lines_char(lines, nb):
    new_lines = []
    for l in lines:
        base = l.split('::')
        new_lines.append(base[0] + '::' + base[1][:nb])
    return new_lines


def allfeature(feature, lines, observation):

    base = lines['lines']
    spe = lines['lines_dot_1']

    observation.append((base + spe).count(feature))

def allpermission(feature, lines, observation):

    base = lines['lines']
    spe = lines['lines_dot_last']

    if feature == 'autre':
        if feature not in spe:
            observation.append(1)
    else:
        observation.append((base + spe).count(feature))


def allactivity(feature, lines, observation):

    base = lines['lines']
    spe = lines['lines_dot_last']

    observation.append((base + spe).count(feature))


def allservice(feature, lines, observation):

    base = lines['lines']
    spe = lines['lines_dot_0']

    if feature == '.':
        observation.append(spe.count(''))
    elif feature == 'autre':
        if feature not in spe:
            observation.append(1)
    else:
        observation.append((base + spe).count(feature))

def allprovider(feature, lines, observation):

    base = lines['lines']
    spe = lines['lines_dot_0']

    if feature == '.':
        observation.append(spe.count(''))
    elif feature == 'autre':
        if feature not in spe:
            observation.append(1)
    else:
        observation.append((base + spe).count(feature))

def allintent(feature, lines, observation):

    base = lines['lines']
    spe = lines['lines_dot_last']

    if feature == 'autre':
        if feature not in spe:
            observation.append(1)
    else:
        observation.append((base + spe).count(feature))

def allcall(feature, lines, observation):

    base = lines['lines']
    spe = lines['lines_3_char']
    spe2 = lines['lines_6_char']

    if feature == 'autre':
        if feature not in (spe+spe2):
            observation.append(1)
    else:
        observation.append((base + spe + spe2).count(feature))

def allreal(feature, lines, observation):

    base = lines['lines']
    spe = lines['lines_dot_0']

    if feature == '.':
        observation.append(spe.count(''))
    elif feature == 'autre':
        if feature not in spe:
            observation.append(1)
    else:
        observation.append((base + spe).count(feature))

def allapi(feature, lines, observation):

    base = lines['lines']
    spe = lines['lines_arrow_last']

    if feature == 'autre':
        if feature not in spe:
            observation.append(1)
    else:
        observation.append((base + spe).count(feature))


feature_set = {
    'feature': allfeature,  # Hardware Components 72
    'permission': allpermission,  # Requested Permission 3812
    'activity': allactivity,  # App Components p1 185729
    'service_receiver': allservice,  # App Component p2 33222
    'provider': allprovider,  # App Component p3 : 4513
    'intent': allintent,  # Filtered Intent 6379
    'call': allcall,  # Restricted Api Calls 733
    'real_permission': allreal,  # Used Permission 70
    'api_call': allapi,  # = Suspicious Api Calls 315
    'url': nonef,  # Network Address 310488
}

EMBEDDING_SIZE = 200
dataset_path = 'Drebin-Dataset'
#features_path = 'Drebin-Dataset/generated/distinct-features_1196.p'
features_path = '../Drebin-Dataset/generated/features_select.p'
labels_path = '../Drebin-Dataset/sha256_family.csv'
in_directory = '../../Data/feature_vectors'
observations_per_file = 132000
out_directory = '../Drebin-Dataset/observations/'
urls_path = '../Drebin-Dataset/generated/urls_310447.p'
startwith = 0

def generate_vectors(features_path=features_path, labels_path=labels_path, in_directory=in_directory,
                     observations_per_file=observations_per_file, out_directory=out_directory, 
                     urls_path = urls_path, topclasses=0):
    if not isdir(out_directory):
        mkdir(out_directory)
    with open(features_path, 'rb') as file:
        features = load(file) # Valeurs possibles des features 

    if isfile(urls_path): # Get the distinct values of urls
        with open(urls_path, 'rb') as f:
            urls = pd.Series(data=load(f))
        emb = MultipleUrlEmbedding(urls, 'feature_extractor/encoder.h5')
        autoencoder, encoder = emb.define_LSTM_autoencoder(EMBSIZE, EMBEDDING_SIZE)
        emb.train_LSTM_autoencoder(autoencoder, encoder, steps=500, epochs=10)
        encoder = load_model(emb.getEncoderPath())
        url = True
    else:
        print("No url file found. Continuing without urls embeddings")
        url = False

    if labels_path is not None:
        p_observations = pandas.read_csv(labels_path,index_col='sha256')
        if topclasses == 0:
            p_observations = p_observations.assign(nbr_class=1)
        else:
            top = list(p_observations.family.value_counts().head(topclasses).keys()) # Classes les plus représentées
            p_observations = p_observations.assign(nbr_class=lambda x: x.family.apply(lambda y: top.index(y) + 1 if y in top else len(top) + 1))

    paths = [join(in_directory, f) for f in listdir(in_directory)][:10000] # chemins des fichiers feature android
    fill_nr = len(str(len(paths) // observations_per_file))
    last_nr = 0
    if isdir(out_directory) and len(listdir(out_directory)) > 0:
        last_nr = sorted(listdir(out_directory))[-1]
        last_nr = int(last_nr.split('.')[0])

    observations = []
    for i in range(startwith, len(paths)):
        path = paths[i]
        print('Observation ' + str(i) + ' of ' + str(len(paths)), path)
        with open(path, 'r') as file:
            observation = []
            lines = file.read().splitlines()
            lines_schema = {
                'lines': lines,
                'lines_dot_last': new_lines_split(lines, '.', -1),
                'lines_dot_0': new_lines_split(lines, '.', 0),
                'lines_dot_1': new_lines_split(lines, '.', 1),
                'lines_arrow_last': new_lines_split(lines, ';->', -1),
                'lines_3_char': new_lines_char(lines, 3),
                'lines_6_char':new_lines_char(lines, 6)
            }

            for feature in features:
                switcher = feature_set.get(feature.split('::')[0], unknown) # Récupère la fonction à utiliser selon la feature
                switcher(feature, lines_schema, observation)
            if url: # Gestion des urls
                allurl(emb, encoder, lines, observation)
            if labels_path is not None: # Si c'est un malware (dans le fichier sha)
                sha = splitext(basename(path))[0]
                if sha in p_observations.index:
                    observation.append(p_observations.loc[sha].nbr_class)
                else:
                    observation.append(0)
            observations.append(observation)
        if (i + 1) % observations_per_file == 0:
            filename = str((i + 1 + last_nr) // observations_per_file).zfill(fill_nr) + '.p'
            with open(join(out_directory, filename), 'wb') as file:
                dump(observations, file)
            observations = []

    if len(observations) > 0:  # Dump the rest observations
        filename = f"SS3_{len(observations[0])}.p" #str(int(ceil(i + last_nr / observations_per_file))) + '.p'
        with open(join(out_directory, filename), 'wb') as file:
            dump(observations, file)


if __name__ == "__main__":
    parser = ArgumentParser(description='Generates python vectors from string observations',
                            formatter_class=ArgumentDefaultsHelpFormatter)
    parser.add_argument('-n', '--number', type=int, default=observations_per_file, help='Number of observations per '
                                                                                        'outputted file. The higher '
                                                                                        'the number, the higher the '
                                                                                        'memory requirements.')
    parser.add_argument('-i', '--input', type=str, default=in_directory, help='The directory in which the input '
                                                                              'observations in their raw string format '
                                                                              'are located. Must be the same directory '
                                                                              'that was also used as input directory for '
                                                                              '`acc_features.py`.')
    parser.add_argument('-l', '--labels', type=str, default=labels_path, help='The path to a CSV file that assigns the '
                                                                              'positive class label to certain '
                                                                              'observations contained in the input '
                                                                              'directory. It contains two columns: '
                                                                              'An observation id column and a malware '
                                                                              'family column. Pass `None` to skip'
                                                                              'labelling')
    parser.add_argument('-f', '--features', type=str, default=features_path,
                        help='The path to the file which contains all '
                             'possible features in a binary format. '
                             'This should be the output file of '
                             '`acc_features.py`.')
    parser.add_argument('-o', '--output', type=str, default=out_directory, help='The directory which the output files '
                                                                                'should be written to.')
    parser.add_argument('-t', '--topclass', type=int, default=9, help='The classification you want (binary=0 / topclass = 1..179 ).'
                                                                      'If you want top 10 classes : 0-9 will be the top classes, 10 will be others and 11 will be benin')
    parser.add_argument('-u', '--urls', type=str, default=urls_path, help='Embeds the Urls instead of juste counting them or treating them as separate features'
                                                                          'File where all the different urls are')
    args = parser.parse_args()
    features_path = args.features
    labels_path = args.labels
    in_directory = args.input
    observations_per_file = args.number
    out_directory = args.output
    top_classes = args.topclass
    urls_path = args.urls
    generate_vectors(features_path, labels_path, in_directory, observations_per_file, out_directory, urls_path, top_classes)
